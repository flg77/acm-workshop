[
{
	"uri": "http://rhacm-ws.nomiras.com/01_cluster_lifecycle/",
	"title": "Cluster Lifecycle",
	"tags": [],
	"description": "",
	"content": " Create a new cluster Cluster Lifecycle (Cluster Lifecycle Management use case) At a high level Cluster Lifecycle management is about creating, upgrading, and destroying and importing clusters in a multi cloud environment. In the 2nd email you received from the RHDP system, it included AWS credentials, Access Key ID, Secret Access Key, and the Base DNS Domain. In order to create a new OpenShift cluster in the AWS cloud we will need these keys to create a Provider Connection.\nNew Single Node Creating a Single Node Cluster (SNO) in AWS In this exercise we will show you how to create a single node cluster (OCP 4.13 or above required) in order to save some time and resources when building clusters for testing. ** Please NOTE that provisioning SNO clusters in public clouds is not currently supported, we only support SNO clusters as bare metal, we leverage the public cloud in the example below to showcase the functionality only.\nCreate Hosted Control Plane ** Hosted Control Planes** is a technology preview feature, it shouldn‚Äôt be used in production clusters. In this section, we will enable the feature and deploy a Hosted Cluster on AWS. On the Clusters page, select once again Create Cluster. Select Amazon Web Services and Hosted mode. You‚Äôll face the official instructions on how to create a hosted cluster. 1 - Turn on Hosted Control Planes functionality The hosted control planes feature is available as a technology preview item and it‚Äôs disabled by default.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/",
	"title": "Application Lifecycle",
	"tags": [],
	"description": "",
	"content": " Create App - Application Lifecycle In the previous lab, you explored the Cluster Lifecycle functionality in RHACM. This allowed you to register/import an OpenShift¬Æ cluster to RHACM, which you can now use to deploy applications. Application Lifecycle functionality in RHACM provides the processes that are used to manage application resources on your managed clusters. This allows you to define a single or multi-cluster application using Kubernetes specifications, but with additional automation of the deployment and lifecycle management of resources to individual clusters.\nDeploy Ansible Automation Platform 2 (AAP2) Deploying Ansible Automation Platform 2 The purpose of this exercise is to deploy an Ansible Automation Platform and the Automation Controller (formerly known as Ansible Tower) and integrate it with RHACM to perform pre/post tasks within the application lifecycle engine. The prehook and posthook task allows you to trigger an Ansible playbook before and after the application is deployed, respectively. Let‚Äôs get started with installing the Ansible Automation Platform (AAP) operator.\nDeploy a Service Now Developer Instance Deploying a ServiceNow Developer Instance The purpose of this exercise is to take advantage of the Ansible Automation Platform (AAP) you just deployed to kick off Ansible Jobs (tied to prehook or posthook tasks in the application lifecycle of ACM. A pre hook will kick off an Ansible job before the application creation / modification. A post hook will kick off an Ansible job after the application creation / modification.\nTrigger a ServiceNow Change Request via Ansible Create an application to take advantage of the ACM - Ansible integration The purpose of this short section is to show how ACM integration with Ansible will kick off an Ansible Job. In this case the Ansible Job will run a playbook that will trigger the creation of a ServiceNow Change Request, exactly like we did and saw in the previous section. Within ACM navigate to the Applications menu on the left, and click Create application ‚Üí\nCreate Ansible Automation Platform Credentials Create AAP credential in ACM Here, we are going to set up the credential which is going to allow ACM to interact with our AAP instance. Click on Credentials on the left menu and select Add Credential button. Credential type: Ansible Automation Platform Credential name: aapaccess Namespace: open-cluster-management Click NEXT For Ansible Tower Host enter you Ansible instance URL, you will find this information on previous screen if not check the ROUTES on your HUB Cluster For Ansible Tower token enter the admin user token you generated earlier In order to save, click NEXT and ADD.\nIntegrated Application Deployment [ACM/AAP2] Create an application to take advantage of the ACM - Ansible integration The purpose of this short section is to show how ACM integration with Ansible will kick off an Ansible Job. In this case the Ansible Job will run a playbook that will trigger the creation of a ServiceNow Change Request, exactly like we did and saw in the previous section. Within ACM navigate to the Applications menu on the left, and click Create application ‚Üí Subscription.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/03_governance/",
	"title": "RHACM Governance",
	"tags": [],
	"description": "",
	"content": "Red Hat Advanced Cluster Management for Kubernetes Governance RHACM Policies Governance, Risk, and Compliance (Security and compliance use case) Creating Policies in ACM At this point, you have completed the overview labs for Cluster Lifecycle and Application Lifecycle capabilities in RHACM. In the Cluster Lifecycle Lab, you learned how RHACM can help manage the lifecycles of your Kubernetes clusters, including both deploying new clusters and importing existing clusters. In that lab, you configured your RHACM instance to manage an OpenShift¬Æ cluster.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/04_observability/",
	"title": "Observability",
	"tags": [],
	"description": "",
	"content": "Explore the observability features of Red Hat Advanced Cluster Management for Kubernetes Grafana Integration Integrate Grafana into ACM Login to the bastion host host. Create a namespace by running the following command: oc create namespace open-cluster-management-observability Copy the pull secret into this new namespace by running the following TWO commands: DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=- open-cluster-management-observability --from-literal=.dockerconfigjson=\u0026#34;$DOCKER_CONFIG_JSON\u0026#34; --type=kubernetes.io/dockerconfigjson In your current folder create a file called thanos-object-storage.yaml and add the following text in the file. Please be sure to update your S3 bucket name and AWS Keys apiVersion: v1 kind: Secret metadata: name: thanos-object-storage type: Opaque stringData: thanos.\nCreate_s3_bucket End to End Visibility (Observability use case) View system alerts, critical application metrics, and overall system health. Search, identify, and resolve issues that are impacting distributed workloads using an operational dashboard designed for Site Reliability Engineers (SREs). This is done via the integration of Grafana. Let\u0026rsquo;s walk through the steps to integrate Grafana with ACM. You will need your AWS Keys. (from RHDP system email) You will also need to create an AWS S3 bucket.\nSummary What is Observability? Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.\nHow does it work? Official Documentation "
},
{
	"uri": "http://rhacm-ws.nomiras.com/06_appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": " Import Kubernetes Importing a Kubernetes cluster You can import previously existing OpenShift clusters, or other supported Kubernetes clusters for example, IKS, EKS, AKS, or GKE. The following is an example of importing an already existing OpenShift cluster into ACM. Click on Add Cluster \u0026ndash;\u0026gt; Import Clusters. Under labels make sure you add the environment=dev label as a label example. Please note that the name you use for the cluster is not relevant, but it makes sense to use the actual cluster name in a production environment.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/01_cluster_lifecycle/01_new_cluster/",
	"title": "Create a new cluster",
	"tags": [],
	"description": "",
	"content": "Cluster Lifecycle (Cluster Lifecycle Management use case) At a high level Cluster Lifecycle management is about creating, upgrading, and destroying and importing clusters in a multi cloud environment. In the 2nd email you received from the RHDP system, it included AWS credentials, Access Key ID, Secret Access Key, and the Base DNS Domain. In order to create a new OpenShift cluster in the AWS cloud we will need these keys to create a Provider Connection. On the left bar, select Credentials and then select Add Credential.\nYou will need to provide connection details\nCredential Type: Choose Amazon Web Services and then, Amazon Web Services again\nCredential Name: aws\nNamespace: open-cluster-management\nBase DNS Domain: This is in the email from the RHDP system\nClick NEXT\nAccess Key ID:\nSecret Access Key ID:\nClick NEXT - We don‚Äôt need to configure a Proxy\nRed Hat OpenShift pull secret: Get from your Red Hat login\nSSH private and public keys: Use an existing key pair or generate a new one (see link below)\nClick NEXT\nVerify the information and click ADD\nPlease refer to Creating a cloud connection for Amazon Web Services for more information on how to complete the step.\nCreate a new OpenShift cluster in AWS From the Clusters page, select Create Cluster Select Amazon Web services and then Standalone. Select the Infrastructure provider credential: aws (may be already selected)\nName: cluster1\nLeave the Cluster set empty for now\nSelect a Release Image, choose a `4.13 version\nAdd a label of environment=prod.\nClick NEXT\nChange the region to see table below\nDepending on the Availability Zone you provisioned your ACM Hub Cluster choose the following regions:\nAvailability Zone (AZ) Region NORTH AMERICA Select us-west-1 or us-west-2 EUROPE / EMEA Select eu-west-2 or eu-west-3 ASIA PACIFIC Select ap-southeast-2 or ap-northeast-2 or ap-east-1 Please note that the deployment might still fail, if so please retry at different availability zone, we have very limited amount of elastic IP addresses in these accounts.\nLeave everything else as is(default) and then click NEXT on the other screens or select 7 - Review and create on the menu and then click CREATE\nIn about 45 minutes this new cluster will be ready to go!\nIf you choose an earlier version of OpenShift, you will also have the option to upgrade OpenShift if you like. This will also take some time.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/01_cluster_lifecycle/02_new_sno/",
	"title": "New Single Node",
	"tags": [],
	"description": "",
	"content": "Creating a Single Node Cluster (SNO) in AWS In this exercise we will show you how to create a single node cluster (OCP 4.13 or above required) in order to save some time and resources when building clusters for testing.\n** Please NOTE that provisioning SNO clusters in public clouds is not currently supported, we only support SNO clusters as bare metal, we leverage the public cloud in the example below to showcase the functionality only. **\nFrom the Clusters page select Create Cluster\nSelect Amazon Web services and then Standalone.\nSelect the Infrastructure provider credential: aws For name: cluster2-sno or add the desired cluster name. Leave the ClusterSet empty for now Select a Release Image, choose a OCP 4.13 version Add a label of environment=qa. Click NEXT Change the region to see table below\nAvailability Zone (AZ) Region NORTH AMERICA Select us-west-1 or us-west-2 EUROPE / EMEA Select eu-west-2 or eu-west-3 ASIA PACIFIC Select ap-southeast-2 or ap-northeast-2 or ap-east-1 Please note that the deployment might still fail, if so please retry at different availability zones, we have a very limited amount of elastic IP addresses in these accounts.\nClick on Worker Pool 1 and change the Node count to 0\nProceed with everything else as is and while at final step, before proceeding turn YAML: ON\nClick on ‚Äúinstall-config‚Äù in the YAML window pane and change the master replica number to 1 (will likely be 3). Double check that the worker replica is 0.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/01_cluster_lifecycle/03_hosted_control_plane/",
	"title": "Create Hosted Control Plane",
	"tags": [],
	"description": "",
	"content": "** Hosted Control Planes** is a technology preview feature, it shouldn‚Äôt be used in production clusters.\nIn this section, we will enable the feature and deploy a Hosted Cluster on AWS.\nOn the Clusters page, select once again Create Cluster.\nSelect Amazon Web Services and Hosted mode. You‚Äôll face the official instructions on how to create a hosted cluster.\n1 - Turn on Hosted Control Planes functionality\nThe hosted control planes feature is available as a technology preview item and it‚Äôs disabled by default. First,have a look in the email instructions on how to log in to the bastion machine, and let‚Äôs start the setup.\nIn the bastion VM, run: aws configure\nWhen prompted, enter your AWS credentials according to the RHDP email you received. For the default region name, use: us-east-2. Leave the default output format blank.\nNext, let‚Äôs create a S3 bucket with public access to host OIDC discovery documents for your hypershift clusters. Run:\n$ export BUCKET_NAME=hcp-oidc $ export REGION=us-east-2 $ aws s3api create-bucket --bucket $BUCKET_NAME \\ --create-bucket-configuration LocationConstraint=$REGION \\ --region $REGION $ aws s3api delete-public-access-block --bucket $BUCKET_NAME $ echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${BUCKET_NAME}/*\u0026#34; } ] }\u0026#39; | envsubst \u0026gt; policy.json $ aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json Now, let‚Äôs create a Secret in the local-cluster namespace so the HyperShift operator will know how to use the bucket that we just created. Don‚Äôt forget the second command to apply the labels:\n$ oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=$BUCKET_NAME --from-literal=region=us-east-2 -n local-cluster $ oc label secret hypershift-operator-oidc-provider-s3-credentials cluster.open-cluster-management.io/credentials=\u0026#34;\u0026#34; cluster.open-cluster-management.io/type=awss3 -n local-cluster You can validate it in the ACM UI, just click on Credentials:\nFinally, we‚Äôre now ready to enable the feature. Let‚Äôs do it by patching the MultiClusterEngine CR.\n$ oc patch mce multiclusterengine --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;overrides\u0026#34;:{\u0026#34;components\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;hypershift-preview\u0026#34;,\u0026#34;enabled\u0026#34;: true}]}}}\u0026#39; Enabling the feature automatically enables the hypershift-addon on local-cluster, per image below. This will also install the HyperShift operator in the hypershift namespace. The installation may take a few minutes to complete.\nNOTE: As mentioned, the hypershift add-on will be automatically installed in the local-cluster. Access the Add-ons tab in the Clusters view and you‚Äôll see the degraded status. As you can see in the image below:\nThis is because the operator will try to deploy 2 operator pods, but this is a single-node OpenShift environment. The anti-affinity rule will make the second pod stay as ‚Äòpending‚Äô. In the OpenShift console, go to the Deployments view in the hypershift project and manually scale it down to 1. This won‚Äôt affect the deployment of the HostedCluster, and consequently won‚Äôt affect the demonstration. See image below:\nYou should now see all the add-ons back to the available status in the local-cluster add-ons tab.\n2 - Deploying the Hosted Cluster First of all, we‚Äôll need to copy your pull secret from this link.\nIn the bastion machine, paste the content of your pull secret in a file called pull-secret in the home directory. Example:\n$ vim $HOME/pull-secret Next, we‚Äôll need the hypershift CLI. From the OpenShift Container Platform console, in the right-hand side of the top bar, click the Help icon (?) -\u0026gt; Command Line Tools.\nRight-click Download hypershift CLI for Linux for x86_64, and click on Copy Link Address or run:\n$ oc get consoleclidownload hypershift-cli-download -o json | jq -r \u0026#39;.spec.links[] | select(.text==\u0026#34;Download hypershift CLI for Linux for x86_64\u0026#34;).href\u0026#39; Now, from the bastion machine, paste the link you just got into the below command and run the following command:\n$ wget \u0026lt;the link copied in the step above\u0026gt; Example: $ wget https://hypershift-cli-download-multicluster-engine.example.com/linux/amd64/hypershift.tar.gz\nYou should see a new file named hypershift.tar.gz in the lab-user home directory. Now, run:\n$ tar xvzf hypershift.tar.gz $ chmod +x ./hypershift $ sudo mv ./hypershift /usr/local/bin/ Finally, let‚Äôs create a namespace to host our HostedCluster and NodePools objects.\n$ oc new-project hcp-clusters Export the following variables:\n$ export CLUSTER_NAME=hcp export INFRA_ID=hcp export NAMESPACE=hcp-clusters export BASE_DOMAIN=\u0026lt;\u0026lt;replace here with your base domain from the RHDP email\u0026gt;\u0026gt; And run:\n$ hypershift create cluster aws \\ --name $CLUSTER_NAME \\ --infra-id $INFRA_ID \\ --namespace $NAMESPACE \\ --base-domain $BASE_DOMAIN \\ --pull-secret $HOME/pull-secret \\ --node-pool-replicas=3 \\ --aws-creds $HOME/.aws/credentials \\ --region $REGION \\ --release-image quay.io/openshift-release-dev/ocp-release:4.13.12-x86_64 NOTE: The command itself might finish quickly, and soon, you‚Äôll see the cluster listed in the Clusters page, but the whole installation process takes around 15 minutes to complete.\nNotice that it says ‚ÄúPending import‚Äù. That‚Äôs ok for now but you‚Äôll need to manually import the cluster once the installation is finished. Now, click on the cluster entry (hcp) to display some details, information and outputs to verify the installation progress.\nAfter a little while, once you see all the configs are reporting ready ‚úÖ - all Cluster Operators are successfully deployed, ClusterVersion is available, NodePool is ready - go ahead to the bottom of the page and you should be able to access the Console URL with the provided kubeadmin credentials.\nNote: some failing status may be also acceptable depending on the hosted cluster configuration. For example: for ClusterVersionUpgradeable, showing a ‚ùìred icon could be normal depending if the next version available has API deprecations.\nDon‚Äôt forget to import the cluster in the cluster‚Äôs view, so RHACM will install its agent in the Hosted Cluster. If everything went successfully, at the end, should see something like this:\nBonus: you can see the containerized control plane by accessing the Topology page in the Developer view and selecting the namespace hcp-clusters-hcp.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/01_create_app/",
	"title": "Create App - Application Lifecycle",
	"tags": [],
	"description": "",
	"content": "In the previous lab, you explored the Cluster Lifecycle functionality in RHACM. This allowed you to register/import an OpenShift¬Æ cluster to RHACM, which you can now use to deploy applications.\nApplication Lifecycle functionality in RHACM provides the processes that are used to manage application resources on your managed clusters. This allows you to define a single or multi-cluster application using Kubernetes specifications, but with additional automation of the deployment and lifecycle management of resources to individual clusters. An application designed to run on a single cluster is straightforward and something you ought to be familiar with from working with OpenShift fundamentals. A multi-cluster application allows you to orchestrate the deployment of these same resources to multiple clusters, based on a set of rules you define for which clusters run the application components.\nThe table below describes the different components that the Application Lifecycle model in RHACM is composed of:\nResource Purpose Channel Defines a place where deployable resources are stored, such as an object store, Kubernetes namespace, Helm repository, or GitHub repository. Subscription Defines a set of deployable resources that are to be deployed to a target cluster. Placement(Old PlacementRule API to be deprecated soon) Defines the target clusters where subscriptions deploy and maintain the application. It is composed of Kubernetes resources identified by the Subscription resource and pulled from the location defined in the Channel resource. Application A way to group the components here into a more easily viewable single resource. An Application resource typically references a Subscription resource. These are all Kubernetes custom resources, defined by a Custom Resource Definition (CRD), that are created for you when RHACM is installed. By creating these as Kubernetes native objects, you can interact with them the same way you would with a Pod. For instance, running oc get application retrieves a list of deployed RHACM applications just as oc get pods retrieves a list of deployed Pods.\nThis may seem like a lot of extra resources to manage in addition to the deployables that actually make up your application. However, they make it possible to automate the composition, placement, and overall control of your applications when you are deploying to many clusters. With a single cluster, it is easy to log in and run oc create -f‚Ä¶‚Äã. If you need to do that on a dozen clusters, you want to make sure you do not make a mistake or miss a cluster, and you need a way to schedule and orchestrate updates to your applications. Leveraging the Application Lifecycle Builder in RHACM allows you to easily manage multi-cluster applications.\nCreating an Application Prerequisites:\nOn the local cluster add a label: `environment=dev` On the new cluster you provisioned via ACM double check you added the label: `environment=prod` In RHACM, navigate to Applications and click Create application and select Subscription. Enter the following information:\nNext to Create application, make sure the YAML dial is ON\n|Name:| book-import| |Namespace:| book-import| ** Under Repository location for resources, select the GIT repository ** URL: https://github.com/hichammourad/book-import.git Branch: master-no-pre-post Path: book-import Under the Select clusters for application deployment, select Deploy application resources on clusters with all specified labels\n|Cluster sets:| global| |Label:| environment| |Value:| dev| Click Create and after a few minutes you will see the application and all its components available in RHACM.\nIf everything was done correctly you should be able to see the application deployed to local-cluster. Go to Applications, and make sure to filter by subscription as the image below:\nThis will show only the apps deployed from ACM, instead of all the existing apps in the managed clusters. Click on the book-import application and have a look at the topology view\nSelect the Route and click on the URL provided, you should see the Book Import application\nSee the Book Import user interface.\nFeel free to experiment with the application. Edit it and change the label to environment=prod. What happens to the application?\nYou have now completed the overview of the Application Lifecycle functionality* in RHACM.\nYou successfully deployed an application to a target cluster using RHACM. This approach leveraged a Git repository which housed all of the manifests that defined your application. RHACM was able to take those manifests and use them as deployables, which were then deployed to the target cluster.\nYou also leverage the power of labels and deploy the application to your imported cluster. I highly encourage you to play around with the labels and deploy this application to your local cluster. You can also create other clusters and or applications if you so desire.\nIf you would like to see how the Ansible Automation Platform ‚Äúprehook‚Äù and ‚Äúposthook‚Äù work inside ACM, you can complete the next section including the ServiceNow section. If you don‚Äôt want to experiment or demo this integration, you can skip to the Creating Policies in ACM section.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/02_deploy_aap/",
	"title": "Deploy Ansible Automation Platform 2 (AAP2)",
	"tags": [],
	"description": "",
	"content": " Deploying Ansible Automation Platform 2 The purpose of this exercise is to deploy an Ansible Automation Platform and the Automation Controller (formerly known as Ansible Tower) and integrate it with RHACM to perform pre/post tasks within the application lifecycle engine. The prehook and posthook task allows you to trigger an Ansible playbook before and after the application is deployed, respectively. Let‚Äôs get started with installing the Ansible Automation Platform (AAP) operator.\n1. Install the Operator For this exercise, we‚Äôll start by accessing the Operator Hub in the OpenShift console(local-cluster/ACM Hub Cluster) to deploy AAP. So, for this, go to the Operator Hub and select the Ansible Automation Platform.\nMake sure to select channel stable-2.4-cluster-scoped and click INSTALL. After a couple of minutes, the APP operator installation will finish. Verify that pods are running by either accessing Workloads ‚Üí Pods under the AAP namespace, or by running this command in the bastion machine:\noc get pods -n aap There should be 4 pods in a running state.\nDeploy the Automation Controller After the operator is successfully deployed, we‚Äôll need to have the Automation Controller instance. Access the AAP operator dashboard(you can do this by clicking on Installed Operators and clicking on the AAP Operator), click in the Automation Controller tab. Then click on Create AutomationController:\nNow, a form will be presented to you. Just give your AutomationController instance a name, like aap2, and hit CREATE. This installation will also take a couple of minutes. Verify this installation, again, by checking the pods in the aap namespace. Now, there should be 7 pods in a running state.\noc get pods -n aap 3. Access the AAP dashboard If everything was done correctly, shortly we‚Äôll be able to see an AAP route deployed. Before accessing the URL, still in the aap namespace, navigate over to Secrets and get the AAP admin password in the secret -admin-password. In this case, app2-admin-password.\nClick on the secret name and you will be able to copy the password at the bottom of the following page.\nNow we‚Äôre all set. Access Networking ‚Üí Routes, and click on the AAP route.\nNote: you may see that AAP is starting up and upgrading components, while the WEB UI is setting up. This should just take less than a minute. Use ‚Äúadmin‚Äù as username and the retrieved secret as password to login to AAP.\n4. Configure AAP Now that you‚Äôre logged in, let‚Äôs proceed with the initial configuration. First of all, when you click on the AAP url, you will be presented with a Subscription dialog window. Click on username/password and provide your Red Hat credentials(from your account in access.redhat.com[link], not Red Hat SSO). You‚Äôll need an Ansible subscription in order to be able to use it. Feel free to use one of your trials and/or the employee SKU.\nSelect one and click NEXT.\nDisable the User Analytics and the Automation Analytics for AAP, we don‚Äôt need those for our example. Click NEXT.\nLastly, accept the end user license agreement by clicking on SUBMIT.\n5. Generate a token for ACM If everything was done correctly it will redirect to the controller page. In the Ansible dashboard, generate a token for the admin user. Go to Users, click admin, select TOKENS, then click the ADD button. Add a short description ‚ÄúToken for use by ACM‚Äù, update the SCOPE to Write, then click SAVE.\nATTENTION! Save the token value to a text file, you will need this token later.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/03_deploy_snow_dev/",
	"title": "Deploy a Service Now Developer Instance",
	"tags": [],
	"description": "",
	"content": "Deploying a ServiceNow Developer Instance The purpose of this exercise is to take advantage of the Ansible Automation Platform (AAP) you just deployed to kick off Ansible Jobs (tied to prehook or posthook tasks in the application lifecycle of ACM.\nA pre hook will kick off an Ansible job before the application creation / modification. A post hook will kick off an Ansible job after the application creation / modification.\nIn the following example the Ansible job will trigger the creation of ServiceNow Change Requests. Perform this step if you would like to show or learn how this integration works. You will take advantage of this within the application lifecycle engine.\nTo deploy your own instance of ServiceNow, please visit https://developer.servicenow.com Select Sign up and Start Building\nAfter signing up and requesting your developer ServiceNow instance (it may look like this You can click on the third option, Developer Program, and then on Start Building üëá)\nYou can select the Utah version(latest) and soon, you will have an instance URL that looks like this: https://dev#####.service-now.com Make note of your instance URL, your admin username and password. Configure Ansible to trigger a job that will create a ServiceNow Change Request Create a ServiceNow Credential Type In Ansible Automation Platform we need to create a Credential Type for ServiceNow, and then add the Credentials for our ServiceNow developer instance.\nSelect Credential Types, under Administration, in the left menu and then click ADD. Name: ServiceNow Input Configuration: fields: - id: SNOW_USERNAME type: string label: Service Now Username - id: SNOW_INSTANCE type: string label: Service Now Instance Name (https://devXXXXX.service-now.com) - id: SNOW_PASSWORD type: string label: Service Now Password secret: true required: - SNOW_USERNAME - SNOW_INSTANCE - SNOW_PASSWORD Add the following text to the INJECTOR CONFIGURATION field extra_vars: snow_instance: \u0026#39;{{ SNOW_INSTANCE }}\u0026#39; snow_password: \u0026#39;{{ SNOW_PASSWORD }}\u0026#39; snow_username: \u0026#39;{{ SNOW_USERNAME }}\u0026#39; SAVE this new credential type Add the ServiceNow Credentials\nNavigate to \u0026ndash;\u0026gt; Resources ‚Üí Credentials. \u0026ndash;\u0026gt; Click on ADD.\n|Name: |ServiceNow Credentials| |CREDENTIAL TYPE| select ServiceNow| |ORGANIZATION | select Default| |USERNAME| admin| |INSTANCE NAME| instance name of your developer instance. (https://dev#####.service-now.com)| |PASSWORD| password from your user settings in the ServiceNow dashboard/website.|\n\u0026ndash;\u0026gt; Then SAVE.\nCreate an Ansible Project In Ansible we need to create a Project\n‚óè Select Projects, under Resources, in the left menu. Click on ADD. Enter snow-create-change-record in the NAME field ‚óè Set the ORGANIZATION field to Default ‚óè Set the SCM TYPE to Git ‚óè Set the SCM URL to: https://github.com/levenhagen/ansible_snow ‚óè Click SAVE\nCreate an Ansible Job Template In Ansible we need to create a Job Template\n‚óè Select Templates, under Resources, in the left menu. ‚óè Click on ADD then Add Job Template. ‚óã NAME: snow-create-change-record ‚óã INVENTORY field search and select Demo Inventory, and check PROMPT ON LAUNCH ‚óã PROJECT: snow-create-change-record ‚óã PLAYBOOK: select servicenow_changerequest.yml ‚óã CREDENTIALS: from the dropdown select ServiceNow, and select ServiceNow Credentials Click SELECT ‚óã VARIABLES field check PROMPT ON LAUNCH ‚óè Scroll all the way to the bottom, leave everything as default and click SAVE ‚óè Success will indicate it can communicate with ServiceNow and it successfully created a Change Request in ServiceNow\nTo verify all was configured properly click the Launch Button. Click NEXT, NEXT and LAUNCH. Verify that the job ran successfully by looking at the output:\nYou can also look in your ServiceNow instance.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/04_snow_change_request/",
	"title": "Trigger a ServiceNow Change Request via Ansible",
	"tags": [],
	"description": "",
	"content": "Create an application to take advantage of the ACM - Ansible integration The purpose of this short section is to show how ACM integration with Ansible will kick off an Ansible Job.\nIn this case the Ansible Job will run a playbook that will trigger the creation of a ServiceNow Change Request, exactly like we did and saw in the previous section.\nWithin ACM navigate to the Applications menu on the left, and click Create application ‚Üí\nSubscription. Enter the following information:\nName: book-import2 Namespace: book-import2 Under repository types, select the GIT repository URL: https://github.com/levenhagen/book-import.git`` Branch: prehook Path: `book-import Expand the ‚ÄúConfigure automation for prehook and posthook‚Äù dropdown menu. Ansible Automation Platform credential: aapaccess Select Deploy application resources only on clusters matching specified labels Label: environment Value: dev\nSAVE the application. Give this a few minutes. The application will complete and in the application topology view you will see the Ansible prehook, and you can infer that it‚Äôs a ServiceNow change request creation.\nWould you like to learn more about the ACM - Ansible integration? Please visit the official documentation for RHACM located here.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/05_create_aap_creds/",
	"title": "Create Ansible Automation Platform Credentials",
	"tags": [],
	"description": "",
	"content": "Create AAP credential in ACM Here, we are going to set up the credential which is going to allow ACM to interact with our AAP instance. Click on Credentials on the left menu and select Add Credential button.\nCredential type: Ansible Automation Platform Credential name: aapaccess Namespace: open-cluster-management\nClick NEXT\nFor Ansible Tower Host enter you Ansible instance URL, you will find this information on previous screen if not check the ROUTES on your HUB Cluster For Ansible Tower token enter the admin user token you generated earlier In order to save, click NEXT and ADD. You‚Äôll be redirected to the Credentials page.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/03_governance/01_rhacm_policies/",
	"title": "RHACM Policies",
	"tags": [],
	"description": "",
	"content": "Governance, Risk, and Compliance (Security and compliance use case) Creating Policies in ACM At this point, you have completed the overview labs for Cluster Lifecycle and Application Lifecycle capabilities in RHACM. In the Cluster Lifecycle Lab, you learned how RHACM can help manage the lifecycles of your Kubernetes clusters, including both deploying new clusters and importing existing clusters. In that lab, you configured your RHACM instance to manage an OpenShift¬Æ cluster.\nIn the Application Lifecycle Lab, you continued exploring RHACM functionality and learned how to deploy and configure an application. You used the cluster that you added in the first module as the target for deploying an application.\nNow that you have a cluster and a deployed application, you need to make sure that they do not drift from their original configurations. This kind of drift is a serious problem, because it can happen from begining and benevolent fixes and changes, as well as malicious activities that you 38might not notice but can cause significant problems. The solution that RHACM provides for this is the Governance, Risk, and Compliance, or GRC, functionality. Review GRC Functionality\nTo begin, it is important to define exactly what GRC is. In RHACM, you build policies that are applied to managed clusters. These policies can do different things, which are described below, but they ultimately serve to govern the configurations of your clusters. This governance over your cluster configurations reduces risk and ensures compliance with standards defined by stakeholders, which can include security teams and operations teams This table describes the three types of policy controllers available in RHACM along with the remediation mode they support:\nPolicy Controller Purpose Enforce or Inform Configuration Used to configure any Kubernetes resource across your clusters. Where these resources are created or configured is determined by the namespaces you include (or exclude) in the policy. Both Certificate Used to detect certificates that are close to expiring. You can configure the certificate policy controller by updating the minimum duration parameter in our controller policy. When a certificate expires in less than the minimum duration, the policy becomes noncompliant. Certificates are identified from secrets in the ncluded namespaces. Inform Identity and Access Management (IAM)The IAM policy controller monitors for the desired maximum number of users with a particular cluster role (i.e. ClusterRole) in our cluster. Inform \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- You need to create three different resources in order to implement the policy controllers:\nResource Function Policy The Policy defines what you actually want to check and possibly configure (with enforce). Policies include a policy-template which defines a list of objectDefinitions. The policy also determines the namespaces it is applied to, as well as the remediation actions it takes. PlacementRule Identifies a list of managed clusters that are targeted when using this PlacementRule. PlacementBinding Connect the policy to the PlacementRule. You can see most of these components running in the hub cluster in the diagram below. RHACM uses all of these to determine which managed clusters and namespaces the policies are applied to.\nFortunately, the RHACM console provides an easy way to start creating basic policies, as shown below. In this example, you can see that when you change values for elements in the RHACM console, the YAML content is updated.\nThis is a complex and emerging/evolving topic, and this course is only providing an overview. Please consult the GRC product documentation for more details on any of these policy controllers. We‚Äôll go through a simple example, and create a policy in the default namespace. For this, we‚Äôll need a little setup: Navigate to Clusters and access the ClusterSets tab.\nThis is a complex and emerging/evolving topic, and this course is only providing an overview. Please consult the GRC product documentation for more details on any of these policy controllers. We‚Äôll go through a simple example, and create a policy in the default namespace. For this, we‚Äôll need a little setup: Navigate to Clusters and access the ClusterSets tab.\nClick on the 3-dots button on the right, and then on Edit namespaces bindings and add the default namespace.\nNavigate to the Governance screen and click create policy.\nBuild a policy with the following information:\nName: policy-grc-cert Namespace: default Click NEXT.\nPolicy Templates: Click on Add policy template and select Certificate Management expiration. Click NEXT.\nOn Placement, click on New placement. Select global as the clusterSet.\nLeave everything else as default and click NEXT.\nAs you can see in the Review page, this policy will be applied to every cluster in the global ClusterSet, and it will look for expired certificates in them. If there‚Äôs a certificate that is set to expire in less time than the minimumDuration specification, the policy will inform a not compliant status. Click SUBMIT. Please note that initially it will complain that there is an issue with the policy but shortly after should go green and get a checkmark.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/04_observability/02_grafana_integration/",
	"title": "Grafana Integration",
	"tags": [],
	"description": "",
	"content": "Integrate Grafana into ACM\nLogin to the bastion host host. Create a namespace by running the following command: oc create namespace open-cluster-management-observability Copy the pull secret into this new namespace by running the following TWO commands:\nDOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-\nopen-cluster-management-observability --from-literal=.dockerconfigjson=\u0026#34;$DOCKER_CONFIG_JSON\u0026#34; --type=kubernetes.io/dockerconfigjson In your current folder create a file called thanos-object-storage.yaml and add the following text in the file. Please be sure to update your S3 bucket name and AWS Keys apiVersion: v1 kind: Secret metadata: name: thanos-object-storage type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: YOUR_S3_BUCKET endpoint: s3.amazonaws.com insecure: false access_key: YOUR_ACCESS_KEY secret_key: YOUR_SECRET_KEY Create a secret for your object storage by running the following command: oc create -f thanos-object-storage.yaml -n open-cluster-management-observability\nCreate the MultiClusterObservability custom resource for your managed clusters. To do this create a YAML file named multiclusterobservability_cr.yaml apiVersion: observability.open-cluster-management.io/v1beta2 metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: key: thanos.yaml name: thanos-object-storage``` Note: This is the default operation. There are multiple other optional fields to customize this resource. If you want to change optional parameters like retentionResolution, storageClass to use, etc., please check the API reference. - Apply the observability YAML to your cluster by running the following command: ```oc apply -f multiclusterobservability_cr.yaml``` - Log in to the ACM console, and navigate to Observe environments -\u0026gt; Overview. - Click on the Grafana link in the top right to view the metrics from the managed clusters. ** Please note:** it will take a few minutes for the metrics to become visible on the dashboard. ![Grafana Cluster](/images/401_1.png) "
},
{
	"uri": "http://rhacm-ws.nomiras.com/06_appendix/01_import_kubernetes_cluster/",
	"title": "Import Kubernetes",
	"tags": [],
	"description": "",
	"content": "Importing a Kubernetes cluster You can import previously existing OpenShift clusters, or other supported Kubernetes clusters for example, IKS, EKS, AKS, or GKE. The following is an example of importing an already existing OpenShift cluster into ACM. Click on Add Cluster \u0026ndash;\u0026gt; Import Clusters. Under labels make sure you add the environment=dev label as a label example. Please note that the name you use for the cluster is not relevant, but it makes sense to use the actual cluster name in a production environment. Once finished click NEXT, NEXT and GENERATE CODE.\nFrom a terminal, login to the target Kubernetes cluster you want to import. Then paste and run the command you just copied. You should see this in your terminal window\nNavigate back to ACM and wait for the cluster to become available (should be no more than 5 to 10 minutes).\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/",
	"title": "RHACM",
	"tags": [],
	"description": "",
	"content": "Red Hat Advanced Cluster Management for Kubernetes (RHACM) AGENDA Time Description 9:00 - 9:15 Welcome and Introductions 9:15 - 9:45 RHACM Overview 9:45 - 10:30 Labtime I - Cluster Lifecycle (Provisioning) 10:30 - 11:30 RHACM Product Manager Christian Start - RHACM Bits \u0026amp; Bytes 11:35 - 12:30 Labtime II 12:30 - 13:30 Lunch 13:30 - 16:00 Labtime III 16:00 - 16:30 Wrap-up and Q\u0026amp;A Workshop Cluster Lifecycle Application Lifecycle RHACM Governance Observability Appendix "
},
{
	"uri": "http://rhacm-ws.nomiras.com/04_observability/01_create_s3_bucket/",
	"title": "Create_s3_bucket",
	"tags": [],
	"description": "",
	"content": "End to End Visibility (Observability use case) View system alerts, critical application metrics, and overall system health. Search, identify, and resolve issues that are impacting distributed workloads using an operational dashboard designed for Site Reliability Engineers (SREs). This is done via the integration of Grafana. Let\u0026rsquo;s walk through the steps to integrate Grafana with ACM. You will need your AWS Keys. (from RHDP system email) You will also need to create an AWS S3 bucket. SSH information to your bastion host. (from RHDP system email) Create the S3 bucket\nLogin to the bastion host. Run the following command to login to AWS: aws configure and enter your AWS keys when prompted. Default region: us-east-2 Then run the following command to create the S3 bucket: aws s3 mb s3://grafana-$GUID\nPlease take note of the bucket name.\nIntegrate Grafana into ACM\nLogin to the bastion host host. Create a namespace by running the following command: oc create namespace open-cluster-management-observability Copy the pull secret into this new namespace by running the following TWO commands:\nDOCKER_CONFIG_JSON=\u0026quot;oc extract secret/pull-secret -n openshift-config --to=-\u0026quot;\noc create secret generic multiclusterhub-operator-pull-secret -n open-cluster-management-observability --from-literal=.dockerconfigjson=\u0026quot;$DOCKER_CONFIG_JSON\u0026quot; --type=kubernetes.io/dockerconfigjson\nIn your current folder create a file called thanos-object-storage.yaml and add the following text in the file. Please be sure to update your S3 bucket name and AWS Keys\nkind: Secret metadata: name: thanos-object-storage type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: YOUR_S3_BUCKET endpoint: s3.amazonaws.com insecure: false access_key: YOUR_ACCESS_KEY secret_key: YOUR_SECRET_KEY``` - Create a secret for your object storage by running the following command: ```oc create -f thanos-object-storage.yaml -n open-cluster-management-observability``` Create the MultiClusterObservability custom resource for your managed clusters. To do this create a YAML file named \u0026#34;multiclusterobservability_cr.yaml\u0026#34; ```kind: MultiClusterObservability apiVersion: observability.open-cluster-management.io/v1beta2 metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: key: thanos.yaml name: thanos-object-storage``` "
},
{
	"uri": "http://rhacm-ws.nomiras.com/02_application_lifecycle/06_integrated_app_deployment/",
	"title": "Integrated Application Deployment [ACM/AAP2] ",
	"tags": [],
	"description": "",
	"content": "Create an application to take advantage of the ACM - Ansible integration The purpose of this short section is to show how ACM integration with Ansible will kick off an Ansible Job.\nIn this case the Ansible Job will run a playbook that will trigger the creation of a ServiceNow Change Request, exactly like we did and saw in the previous section. Within ACM navigate to the Applications menu on the left, and click Create application ‚Üí Subscription.\nEnter the following information:\nName: book-import2\nNamespace: book-import2\nUnder repository types, select the GIT repository\nURL: https://github.com/levenhagen/book-import.git Branch: prehook Path: book-import\nExpand the ‚ÄúConfigure automation for prehook and posthook‚Äù dropdown menu.\nAnsible Automation Platform credential: aapaccess\nSelect Deploy application resources only on clusters matching specified labels Label: environment Value: dev\nSAVE the application. Give this a few minutes. The application will complete and in the application topology view you will see the Ansible prehook, and you can infer that it‚Äôs a ServiceNow change request creation.\n"
},
{
	"uri": "http://rhacm-ws.nomiras.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://rhacm-ws.nomiras.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]